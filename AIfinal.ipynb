{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiLpM5z9OazU"
      },
      "source": [
        "#Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH6katgpZcpq"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "class AutonomousVacuum:\n",
        "    cleaningCost = 20 #cost of cleaning a cell\n",
        "    wallCost = -5 # cost of hitting the wall\n",
        "    batteryCost = -7 #cost of running out of battery\n",
        "    dustStorageCost = -10 # cost of running of dust storage\n",
        "    objectCost = -5 #cost  of hitting any moving objects in the room\n",
        "    recharge = 20 #cost of going back to recharge the vaccume\n",
        "    emptyStorage = 15 #cost of emptying the dust storage\n",
        "    hitWallProb= 0.3\n",
        "    hitObject=0.4\n",
        "\n",
        "    def __init__(self, grid_size, initial_position, base_position):\n",
        "        self.grid_size = grid_size\n",
        "        self.position = initial_position\n",
        "        self.base_position = base_position\n",
        "        self.initial_position = initial_position\n",
        "        self.battery = 100\n",
        "        self.dust_storage = 0\n",
        "        # Initialize all cells as dirty\n",
        "        self.dirty_cells = {(i, j) for i in range(grid_size) for j in range(grid_size)}\n",
        "        # Remove the initial position from the dirty cells if necessary\n",
        "        '''if initial_position in self.dirty_cells:\n",
        "            self.dirty_cells.remove(initial_position)'''\n",
        "\n",
        "\n",
        "    def is_end(self):\n",
        "        # Check if all cells are cleaned\n",
        "        if len(self.dirty_cells) == 1:\n",
        "          return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    def states(self):\n",
        "     return list(itertools.product(range(self.grid_size), range(self.grid_size)))\n",
        "\n",
        "\n",
        "    def actions(self):\n",
        "       # Get possible moves based on the current position\n",
        "       possible_moves = []\n",
        "       x,y = self.position\n",
        "       if x < self.grid_size - 1:\n",
        "           possible_moves.append('down')#x + 1, y\n",
        "\n",
        "       if x > 0 :\n",
        "           possible_moves.append('up')#(x - 1, y)\n",
        "\n",
        "       if y > 0 :\n",
        "           possible_moves.append('left')#(x, y - 1)\n",
        "\n",
        "       if y < self.grid_size - 1:\n",
        "           possible_moves.append('right')#(x, y + 1)\n",
        "\n",
        "       return possible_moves\n",
        "\n",
        "    def succReward(self, action):\n",
        "     result = []  # newStates, probHitWall, probObject, reward\n",
        "     x, y = self.position\n",
        "\n",
        "     if action in self.actions():\n",
        "\n",
        "        if self.battery >= 5 and self.dust_storage <= 10:\n",
        "            newBattery = self.battery - 5\n",
        "            newStorage = self.dust_storage + 10\n",
        "            if action == 'down':\n",
        "                newPosition = (x + 1, y)\n",
        "            elif action == 'up':\n",
        "                newPosition = (x - 1, y)\n",
        "            elif action == 'left':\n",
        "                newPosition = (x, y - 1)\n",
        "            elif action == 'right':\n",
        "                newPosition = (x, y + 1)\n",
        "\n",
        "            hitWall = (newPosition[0] == 0 or newPosition[0] == self.grid_size-1 or\n",
        "                       newPosition[1] == 0 or newPosition[1] == self.grid_size-1)\n",
        "\n",
        "            if hitWall:\n",
        "                newPosition = self.position  # Stay in the same position if it hits the wall\n",
        "                reward = self.wallCost\n",
        "            else:\n",
        "                reward = self.clean(newPosition)\n",
        "\n",
        "            # Calculate expected reward considering the probabilities of hitting wall and objects\n",
        "            result.append((newPosition, newBattery, newStorage, 1 - self.hitWallProb, 1 - self.hitObject, reward))\n",
        "            result.append((newPosition, newBattery, newStorage, 1 - self.hitWallProb, self.hitObject, reward + self.objectCost))\n",
        "            result.append((self.position, newBattery, self.dust_storage, self.hitWallProb, 1 - self.hitObject, self.wallCost))\n",
        "            result.append((self.position, newBattery, self.dust_storage, self.hitWallProb, self.hitObject, self.wallCost + self.objectCost))\n",
        "\n",
        "        elif self.battery <= 5:  # Corrected condition here\n",
        "            # If the battery is too low, the vacuum needs to return to base for recharging\n",
        "            result.append((self.base_position, 100, self.dust_storage, 1, 1, self.recharge + self.batteryCost))\n",
        "\n",
        "        elif self.dust_storage >= 10:\n",
        "            # If the dust storage is full, the vacuum needs to return to base to empty the storage\n",
        "            result.append((self.base_position, self.battery, 0, 1, 1, self.emptyStorage + self.dustStorageCost))\n",
        "\n",
        "     return result\n",
        "\n",
        "\n",
        "\n",
        "    def clean(self, position):\n",
        "        if position in self.dirty_cells:\n",
        "            self.dirty_cells.remove(position)\n",
        "            return self.cleaningCost\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "    def recharge(self):\n",
        "        self.battery = 100\n",
        "        return self.recharge\n",
        "\n",
        "    def emptyStorage(self):\n",
        "        self.dust_storage = 0\n",
        "        return self.emptyStorage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wij6K9CmflJF",
        "outputId": "fe844919-4c5e-49cd-e304-9452a066bbd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[((3, 0), 95, 10, 0.7, 0.6, -5), ((3, 0), 95, 10, 0.7, 0.4, -10), ((3, 0), 95, 0, 0.3, 0.6, -5), ((3, 0), 95, 0, 0.3, 0.4, -10)]\n"
          ]
        }
      ],
      "source": [
        "ip=(3,0)\n",
        "bp=(0,1)\n",
        "state=(0,1)\n",
        "av=AutonomousVacuum(10, ip, bp)\n",
        "\n",
        "print(av.succReward('up'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Policy Evaluation\n"
      ],
      "metadata": {
        "id": "8EBePGqERSiS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgNUTQ17_vNR"
      },
      "outputs": [],
      "source": [
        "def evaluatePolicy(vacuum, policy):\n",
        "    V = {}\n",
        "\n",
        "    # Initialize\n",
        "    for i in range(vacuum.grid_size):\n",
        "      for j in range(vacuum.grid_size):\n",
        "         V[(i, j)] = 0\n",
        "\n",
        "\n",
        "    def Q(position, action):\n",
        "        vacuum.position =position\n",
        "        outcomes = vacuum.succReward(action)\n",
        "        return sum(hitWallProb * hitObjectProb * (reward + V.get(newPosition,0))\n",
        "                    for newPosition, battery, dust_storage, hitWallProb, hitObjectProb, reward in outcomes)\n",
        "\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        newV = {}\n",
        "\n",
        "        for i in range(vacuum.grid_size):\n",
        "           for j in range(vacuum.grid_size):\n",
        "              position = (i, j)\n",
        "              # Create a copy of the dirty_cells set\n",
        "              dirty_cells_copy = vacuum.dirty_cells.copy()\n",
        "              # Check if the position is in the dirty_cells_copy set\n",
        "              if position in dirty_cells_copy:\n",
        "                  dirty_cells_copy.remove(position)\n",
        "              vacuum.dirty_cells = dirty_cells_copy  # Set the vacuum's dirty_cells to the copy\n",
        "              if vacuum.is_end():\n",
        "                newV[position] = 0\n",
        "\n",
        "              else:\n",
        "                  action = policy.get(position, vacuum.actions()[0])\n",
        "                  newV[position] = Q(position, action)\n",
        "                  delta = max(delta, abs(newV[position] - V[position]))\n",
        "\n",
        "        V = newV\n",
        "\n",
        "        if delta < 1e-10:\n",
        "            break\n",
        "\n",
        "    return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "m6qUZ3CV-yr0",
        "outputId": "c6cb6290-e18e-4e5d-88c1-daa51401ee60"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-eba69d77a037>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Evaluate the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluatePolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvacuum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Print the resulting value function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-e6e00cea0d53>\u001b[0m in \u001b[0;36mevaluatePolicy\u001b[0;34m(vacuum, policy)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                   \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvacuum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                   \u001b[0mnewV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                   \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-cc11a7aeee59>\u001b[0m in \u001b[0;36mactions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m            \u001b[0mpossible_moves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(x, y + 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mpossible_moves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define the grid size, initial position, and base position\n",
        "grid_size = 3\n",
        "initial_position = (0, 0)\n",
        "base_position = (0, 0)\n",
        "\n",
        "# Instantiate the AutonomousVacuum object\n",
        "vacuum = AutonomousVacuum(grid_size, initial_position, base_position)\n",
        "\n",
        "# Define a simple policy that always tries to move 'down' if possible, otherwise 'right'\n",
        "policy = {\n",
        "    (0, 0): 'down',\n",
        "    (0, 1): 'down',\n",
        "    (0, 2): 'down',\n",
        "    (1, 0): 'down',\n",
        "    (1, 1): 'down',\n",
        "    (1, 2): 'down',\n",
        "    (2, 0): 'right',\n",
        "    (2, 1): 'right',\n",
        "    (2, 2): 'down',  # This is a terminating state, no action should be needed\n",
        "}\n",
        "\n",
        "# Evaluate the policy\n",
        "V = evaluatePolicy(vacuum, policy)\n",
        "\n",
        "# Print the resulting value function\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        print(f\"V({i}, {j}) = {V[(i, j)]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Optimal Policy"
      ],
      "metadata": {
        "id": "ZMy7qIPHUIds"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAtQcquB-29R"
      },
      "outputs": [],
      "source": [
        "def valueIteration(vacuum):\n",
        "    V = {}  # state -> estimate of V_opt(state)\n",
        "    pi = {}  # state -> estimate of pi_opt(state)\n",
        "\n",
        "    # Initialize\n",
        "    for state in vacuum.states():\n",
        "        V[state] = 0\n",
        "        pi[state] = None\n",
        "\n",
        "    # Based on current estimate of V\n",
        "    def Q(position, action):\n",
        "        vacuum.position =position\n",
        "        outcomes = vacuum.succReward(action)\n",
        "        return sum(hitWallProb * hitObjectProb * (reward + V.get(newPosition,0))\n",
        "                    for newPosition, battery, dust_storage, hitWallProb, hitObjectProb, reward in outcomes)\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        newV = {}\n",
        "\n",
        "        for state in vacuum.states():\n",
        "            vacuum.position=state\n",
        "            if vacuum.isEnd(state):\n",
        "                newV[state] = 0\n",
        "                pi[state] = None\n",
        "            else:\n",
        "                max_action_value = float('-inf')\n",
        "                optimal_action = None\n",
        "                for action in vacuum.actions():\n",
        "                    action_value = Q(state, action)\n",
        "                    if action_value > max_action_value:\n",
        "                        max_action_value = action_value\n",
        "                        optimal_action = action\n",
        "                newV[state] = max_action_value\n",
        "                pi[state] = optimal_action\n",
        "\n",
        "                delta = max(delta, abs(newV[state] - V[state]))\n",
        "\n",
        "        # Test for convergence\n",
        "        if delta < 1e-10:\n",
        "            break\n",
        "\n",
        "        V = newV\n",
        "\n",
        "    return V, pi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "rbnEnujFAAQR",
        "outputId": "c808bce6-26b6-4c28-eee0-201db7f82fb3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6b9ba052d7f2>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Run value iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalueIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvacuum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Print the resulting value function and policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-a97880818de5>\u001b[0m in \u001b[0;36mvalueIteration\u001b[0;34m(vacuum)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0moptimal_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvacuum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     \u001b[0maction_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0maction_value\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_action_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mmax_action_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-a97880818de5>\u001b[0m in \u001b[0;36mQ\u001b[0;34m(position, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvacuum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutcomes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvacuum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccReward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         return sum(hitWallProb * hitObjectProb * (reward + V.get(newPosition,0))\n\u001b[1;32m     15\u001b[0m                     for newPosition, battery, dust_storage, hitWallProb, hitObjectProb, reward in outcomes)\n",
            "\u001b[0;32m<ipython-input-1-594ed74bf8d3>\u001b[0m in \u001b[0;36msuccReward\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Calculate expected reward considering the probabilities of hitting wall and objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewPosition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewBattery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewStorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhitWallProb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhitObject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewPosition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewBattery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewStorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhitWallProb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhitObject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjectCost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewBattery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdust_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhitWallProb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhitObject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallCost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "grid_size = 3\n",
        "initial_position = (0, 0)\n",
        "base_position = (0, 0)\n",
        "\n",
        "# Instantiate the AutonomousVacuum object\n",
        "vacuum = AutonomousVacuum(grid_size, initial_position, base_position)\n",
        "\n",
        "# Run value iteration\n",
        "V, pi = valueIteration(vacuum)\n",
        "\n",
        "# Print the resulting value function and policy\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        position = (i, j)\n",
        "        print(f\"V({position}) = {V[position]:.2f}, pi({position}) = {pi[position]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Optimizing by Monte Carlo"
      ],
      "metadata": {
        "id": "DFGQwFqkUThS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0QF55jFAIzy"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_episode(vacuum, policy):\n",
        "    episode = []\n",
        "    while not vacuum.is_end():\n",
        "        state = vacuum.position\n",
        "        action = policy[state]\n",
        "        next_states = vacuum.succReward(action)\n",
        "        next_state = random.choices([outcome[0] for outcome in next_states], weights=[outcome[3]*outcome[4] for outcome in next_states])[0]\n",
        "        reward = sum(outcome[3]*outcome[4]*outcome[5] for outcome in next_states)\n",
        "        episode.append((state, action, reward))\n",
        "        vacuum.position = next_state\n",
        "    return episode\n",
        "\n",
        "def monte_carlo_control(vacuum, episodes=1000, epsilon=0.1, discount=0.9):\n",
        "    Q = {} #the expected reward for taking action in a state (action - value)\n",
        "    N = {} #to count the number of times each state-action pair is visited\n",
        "    policy = {} #mapping states to actions\n",
        "\n",
        "    for i in range(episodes):\n",
        "        episode = generate_episode(vacuum, policy)\n",
        "        G = 0\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = discount * G + reward\n",
        "            N[(state, action)] = N.get((state, action), 0) + 1\n",
        "            Q[(state, action)] = Q.get((state, action), 0) + (1 / N[(state, action)]) * (G - Q.get((state, action), 0))\n",
        "            best_action = max(vacuum.actions(), key=lambda a: Q.get((state, a), 0))\n",
        "            policy[state] = best_action if random.random() > epsilon else random.choice(vacuum.actions())\n",
        "    return policy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WjoqxtjX4Dvb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}